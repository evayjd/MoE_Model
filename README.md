Minimal PyTorch implementation of Mixture-of-Experts (MoE) architectures.
Includes a dense MoE and a Top-k Sparse MoE with routing and expert masking.
Built for understanding expert routing, token dispatch, and weighted aggregation.
